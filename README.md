### ğŸ“° Fake News Detection AI
## ğŸ“Œ Project Overview
This project demonstrates a fake news detection pipeline using a baseline TF-IDF + Logistic Regression model and an advanced RoBERTa fine-tuning approach with Hugging Face Transformers, with the dataset stored in Google Drive and processed in Google Colab
## ğŸ“‚ Repository Structure
```
fake-news/  
â”‚  
â”œâ”€â”€ config/                  # Config files (optional)  
â”œâ”€â”€ data/  
â”‚   â”œâ”€â”€ raw/                 # Raw CSV dataset  
â”‚   â””â”€â”€ processed/           # Preprocessed JSONL dataset  
â”‚  
â”œâ”€â”€ src/  
â”‚   â”œâ”€â”€ prepare.py           # Data cleaning & preprocessing  
â”‚   â”œâ”€â”€ baseline.py          # TF-IDF + Logistic Regression  
â”‚   â””â”€â”€ transformer_ft.py    # Transformer fine-tuning (RoBERTa)  
â”‚  
â”œâ”€â”€ runs/                    # Model outputs & metrics  
â”‚   â”œâ”€â”€ baseline.json  
â”‚   â””â”€â”€ roberta_metrics.json  
â”‚
â””â”€â”€ app/                     # (Optional) App integration
```

## ğŸ“‚ Dataset Structure
```
data/
â”‚â”€â”€ raw/
â”‚   â””â”€â”€ news.csv             # Contains text + label
â”‚â”€â”€ processed/
â”‚   â””â”€â”€ dataset.jsonl
```
* text â†’ News article content

* label â†’ ```0``` for real news, ```1``` for fake news

## âš™ Environment Setup
```
pip install -U pandas numpy scikit-learn transformers datasets
```
## ğŸš€ Steps to Run
1ï¸âƒ£ Data Preparation
```
python src/prepare.py \
  --input data/raw/news.csv \
  --out data/processed/dataset.jsonl
```
2ï¸âƒ£ Baseline Model (TF-IDF + Logistic Regression)
```
python src/baseline.py \
  --input data/processed/dataset.jsonl \
  --out runs/baseline.json
```
3ï¸âƒ£ Transformer Fine-Tuning (RoBERTa)
```
python src/transformer_ft.py \
  --input data/processed/dataset.jsonl \
  --model_name roberta-base \
  --epochs 2 \
  --out runs/roberta_metrics.json
```
## ğŸ“Š Example Output

Baseline:
```
{"f1": 0.82}
```

Transformer:
```
{"eval_loss": 0.45, "eval_accuracy": 0.87, "eval_f1": 0.85}
```
## ğŸ“Œ Future Improvements

* Increase dataset size for better model performance

* Apply advanced text augmentation techniques

* Experiment with larger models (e.g., roberta-large, deberta-v3-base)
